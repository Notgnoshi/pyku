{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape haikus from different places on the internet\n",
    "\n",
    "URLs used:\n",
    "\n",
    "* [http://www.hsa-haiku.org/hendersonawards/henderson.htm](http://www.hsa-haiku.org/hendersonawards/henderson.htm)\n",
    "* [http://www.hsa-haiku.org/bradyawards/brady.htm](http://www.hsa-haiku.org/bradyawards/brady.htm)\n",
    "* [http://www.hsa-haiku.org/museumhaikuliteratureawards/museumhaikuliterature-award.htm](http://www.hsa-haiku.org/museumhaikuliteratureawards/museumhaikuliterature-award.htm)\n",
    "* [http://www.hsa-haiku.org/virgilioawards/virgilio.htm](http://www.hsa-haiku.org/virgilioawards/virgilio.htm)\n",
    "* [http://sacred-texts.com/shi/jh/index.htm](http://sacred-texts.com/shi/jh/index.htm)\n",
    "* [https://www.thehaikufoundation.org/per-diem-archive/](https://www.thehaikufoundation.org/per-diem-archive/)\n",
    "* [https://www.ahapoetry.com/aadoh/h_dictionary.htm](https://www.ahapoetry.com/aadoh/h_dictionary.htm)\n",
    "\n",
    "This notebook writes the haikus to different `*.txt` files with one haiku per line, with each line of the haiku separated with tabs.\n",
    "\n",
    "Note that the `scrape_data.py` file in this folder is the final result of this notebook. It was just useful to use a notebook to interactively prototype the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine what 'alphabet' means so we don't filter out newlines and spaces.\n",
    "ALPHABET = frozenset(string.ascii_lowercase + '\\n' + ' ')\n",
    "\n",
    "\n",
    "def preprocess(text, use_ascii=True):\n",
    "    \"\"\"\n",
    "        Preprocess text. Converts to lowercase and filters non-alphabetic characters.\n",
    "        Defaults to defining alphabetic characters as ascii-alphabetic\n",
    "        Examples:\n",
    "        >>> text = 'ABC.,#'\n",
    "        >>> ''.join(preprocess(text))\n",
    "        'abc'\n",
    "        >>> text = 'ÈÆÖÉEAEOE,.%'\n",
    "        >>> ''.join(preprocess(text, use_ascii=False))\n",
    "        'èæöéeaeoe'\n",
    "    \"\"\"\n",
    "    if use_ascii:\n",
    "        return filter(ALPHABET.__contains__, text.lower())\n",
    "    return filter(str.isalpha, text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Henderson awarded haikus from [http://www.hsa-haiku.org/hendersonawards/henderson.htm](http://www.hsa-haiku.org/hendersonawards/henderson.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = HTMLSession()\n",
    "r = session.get('http://www.hsa-haiku.org/hendersonawards/henderson.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus = r.html.find('td > blockquote > p')\n",
    "haikus = (h.text for h in haikus)\n",
    "haikus = (''.join(preprocess(h)).strip() for h in haikus)\n",
    "haikus = filter(lambda x: len(x) > 1, haikus)\n",
    "haikus = (h.split('\\n') for h in haikus)\n",
    "haikus = ('\\t'.join(' '.join(line.split()) for line in h if len(line) > 1) for h in haikus)\n",
    "\n",
    "with open('henderson.txt', 'w') as f:\n",
    "    for haiku in haikus:\n",
    "        f.write(haiku + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Brady awarded haikus from [http://www.hsa-haiku.org/bradyawards/brady.htm](http://www.hsa-haiku.org/bradyawards/brady.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get('http://www.hsa-haiku.org/bradyawards/brady.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus = r.html.find('td > blockquote > p')\n",
    "haikus = (h.text for h in haikus)\n",
    "haikus = (''.join(preprocess(h)).strip() for h in haikus)\n",
    "haikus = filter(lambda x: len(x) > 1, haikus)\n",
    "haikus = (h.split('\\n') for h in haikus)\n",
    "haikus = ('\\t'.join(' '.join(line.split()) for line in h if len(line) > 1) for h in haikus)\n",
    "\n",
    "with open('brady.txt', 'w') as f:\n",
    "    for haiku in haikus:\n",
    "        f.write(haiku + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Museum Haiku awarded haikus from [http://www.hsa-haiku.org/museumhaikuliteratureawards/museumhaikuliterature-award.htm](http://www.hsa-haiku.org/museumhaikuliteratureawards/museumhaikuliterature-award.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get('http://www.hsa-haiku.org/museumhaikuliteratureawards/museumhaikuliterature-award.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus = r.html.find('p.haiku')\n",
    "haikus = (h.text for h in haikus)\n",
    "haikus = (''.join(preprocess(h)).strip() for h in haikus)\n",
    "haikus = filter(lambda x: len(x) > 1, haikus)\n",
    "haikus = (h.split('\\n') for h in haikus)\n",
    "haikus = ('\\t'.join(' '.join(line.split()) for line in h if len(line) > 1) for h in haikus)\n",
    "\n",
    "with open('museum.txt', 'w') as f:\n",
    "    for haiku in haikus:\n",
    "        f.write(haiku + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Virgilio awarded haikus from [http://www.hsa-haiku.org/virgilioawards/virgilio.htm](http://www.hsa-haiku.org/virgilioawards/virgilio.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get('http://www.hsa-haiku.org/virgilioawards/virgilio.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus = r.html.find('.haiku')\n",
    "haikus = (h.text for h in haikus)\n",
    "haikus = (''.join(preprocess(h)).strip() for h in haikus)\n",
    "haikus = filter(lambda x: len(x) > 1, haikus)\n",
    "haikus = (h.split('\\n') for h in haikus)\n",
    "haikus = ('\\t'.join(' '.join(line.split()) for line in h if len(line) > 1) for h in haikus)\n",
    "\n",
    "with open('virgilio.txt', 'w') as f:\n",
    "    for haiku in haikus:\n",
    "        f.write(haiku + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get haikus from [http://sacred-texts.com/shi/jh/index.htm](http://sacred-texts.com/shi/jh/index.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_URLS = [f'http://sacred-texts.com/shi/jh/jh0{i}.htm' for i in range(2, 8)]\n",
    "\n",
    "all_haikus = []\n",
    "for url in SUB_URLS:\n",
    "    r = session.get(url)\n",
    "    haikus = r.html.find('td > p')\n",
    "    haikus = (h.text for h in haikus)\n",
    "    haikus = (''.join(preprocess(h)).strip() for h in haikus)\n",
    "    haikus = filter(lambda x: len(x) > 12, haikus)\n",
    "    haikus = (h.split('\\n') for h in haikus)\n",
    "    haikus = ('\\t'.join(' '.join(line.split()) for line in h if len(line) > 1) for h in haikus)\n",
    "    \n",
    "    \n",
    "    all_haikus += list(haikus)\n",
    "\n",
    "with open('sacred.txt', 'w') as f:\n",
    "    for haiku in all_haikus:\n",
    "        f.write(haiku + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get haikus from [https://www.thehaikufoundation.org/per-diem-archive/](https://www.thehaikufoundation.org/per-diem-archive/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get('https://www.thehaikufoundation.org/per-diem-archive/')\n",
    "urls = r.html.find('li > a')\n",
    "urls = (u.attrs['href'] for u in urls)\n",
    "urls = filter(lambda x: 'IDcat' in x, urls)\n",
    "urls = (f'https://www.thehaikufoundation.org{u}' for u in urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_haikus = []\n",
    "for url in urls:\n",
    "    r = session.get(url)\n",
    "    try:\n",
    "        haikus = r.html.find('td > pre')\n",
    "        haikus = (h.text for h in haikus)\n",
    "        haikus = (''.join(preprocess(h)).strip() for h in haikus)\n",
    "        haikus = filter(lambda x: len(x) > 1, haikus)\n",
    "        haikus = (h.split('\\n') for h in haikus)\n",
    "        haikus = ('\\t'.join(' '.join(line.split()) for line in h if len(line) > 1) for h in haikus)\n",
    "\n",
    "        all_haikus += list(haikus)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perdiem.txt', 'w') as f:\n",
    "    for haiku in all_haikus:\n",
    "        f.write(haiku + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get haikus from [https://www.ahapoetry.com/aadoh/h_dictionary.htm](https://www.ahapoetry.com/aadoh/h_dictionary.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get('https://www.ahapoetry.com/aadoh/h_dictionary.htm')\n",
    "urls = r.html.find('p > a')\n",
    "urls = (u.attrs['href'] for u in urls)\n",
    "urls = (f'https://www.ahapoetry.com/aadoh/{u}' for u in urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key(x):\n",
    "    \"\"\"Is a given x a haiku?\"\"\"\n",
    "    try:\n",
    "        return x.attrs['align'] == 'center'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "all_haikus = []\n",
    "for url in urls:\n",
    "    r = session.get(url)\n",
    "    haikus = r.html.find('p')\n",
    "    haikus = filter(key, haikus)\n",
    "    haikus = (h.text for h in haikus)\n",
    "    haikus = (''.join(preprocess(h)).strip() for h in haikus)\n",
    "    haikus = filter(lambda x: len(x) > 1, haikus)\n",
    "    haikus = (h.split('\\n') for h in haikus)\n",
    "    haikus = ('\\t'.join(' '.join(line.split()) for line in h if len(line) > 1) for h in haikus)\n",
    "    \n",
    "    all_haikus += list(haikus)\n",
    "\n",
    "with open('aadoh.txt', 'w') as f:\n",
    "    for haiku in all_haikus:\n",
    "        f.write(haiku + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
